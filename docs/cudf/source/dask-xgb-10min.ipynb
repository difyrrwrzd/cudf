{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 Minutes to distributed GPU accelerated XGBoost using dmlc/xgboost's Dask API\n",
    "\n",
    "The [Dask API of DMLC XGBoost](https://xgboost.readthedocs.io/en/latest/tutorials/dask.html) enables distributed GPU accelerated XGBoost via distributed CUDA DataFrame, Dask-cuDF. A user may pass a reference to a distributed cuDF object, and start a training session over an entire cluster from Python. For a better understanding of this Dask API, please refer to [this article](https://medium.com/rapids-ai/a-new-official-dask-api-for-xgboost-e8b10f3d1eb7)\n",
    "\n",
    "Let's get started.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disable NCCL P2P. Only necessary for versions of NCCL < 2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: NCCL_P2P_DISABLE=1\n"
     ]
    }
   ],
   "source": [
    "%env NCCL_P2P_DISABLE=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary modules and initialize the Dask-cuDF Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `LocalCUDACluster` from Dask-CUDA to instantiate the single-node cluster.\n",
    "\n",
    "A user may instantiate a Dask-cuDF cluster like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nfs/rabhojwani/anaconda3/envs/rapids-env2/lib/python3.7/site-packages/distributed/node.py:155: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 43931 instead\n",
      "  http_address[\"port\"], self.http_server.port\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://10.33.227.157:41735</li>\n",
       "  <li><b>Dashboard: </b><a href='http://10.33.227.157:43931/status' target='_blank'>http://10.33.227.157:43931/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>8</li>\n",
       "  <li><b>Cores: </b>8</li>\n",
       "  <li><b>Memory: </b>540.94 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://10.33.227.157:41735' processes=8 threads=8, memory=540.94 GB>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cudf\n",
    "import cupy\n",
    "import dask\n",
    "import dask_cudf\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from dask.distributed import Client, wait\n",
    "from dask_cuda import LocalCUDACluster\n",
    "\n",
    "import subprocess\n",
    "\n",
    "cmd = \"hostname --all-ip-addresses\"\n",
    "process = subprocess.Popen(cmd.split(), stdout=subprocess.PIPE)\n",
    "output, error = process.communicate()\n",
    "IPADDR = str(output.decode()).split()[0]\n",
    "\n",
    "cluster = LocalCUDACluster(ip=IPADDR)\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the use of `from dask_cuda import LocalCUDACluster`. [Dask-CUDA](https://github.com/rapidsai/dask-cuda) is a lightweight set of utilities useful for setting up a Dask cluster. These calls instantiate a Dask-cuDF cluster in a single node environment. To instantiate a multi-node Dask-cuDF cluster, a user must use `dask-scheduler` and `dask-cuda-worker`. These are utilities available at the command-line to launch the scheduler, and its associated workers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize a Random Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `dask_cudf.DataFrame.query` to split the dataset into train-and-test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 1000000\n",
    "npartitions = 8\n",
    "\n",
    "cdf = cudf.DataFrame({'x': cupy.random.randint(0, npartitions, size=size), 'y': cupy.random.normal(size=size)})\n",
    "ddf = dask_cudf.from_cudf(cdf,npartitions=npartitions)\n",
    "\n",
    "X_train = ddf.query('y < 0.5')\n",
    "Y_train = X_train[['x']]\n",
    "X_train = X_train[X_train.columns.difference(['x'])]\n",
    "\n",
    "X_test = ddf.query('y > 0.5')\n",
    "Y_test = X_test[['x']]\n",
    "X_test = X_test[X_test.columns.difference(['x'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Dask API for XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a DaskDMatrix object from our training input and target data using `xgb.dask.DaskMatrix`\n",
    "\n",
    "The DaskDMatrix constructor forces all lazy computation to materialize. To isolate the computation in DaskDMatrix from other lazy computations, one can use wait. Please note: this is optional. For more details, [refer here](https://xgboost.readthedocs.io/en/latest/tutorials/dask.html#why-is-the-initialization-of-daskdmatrix-so-slow-and-throws-weird-errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Optional: this wont return until all data is in GPU memory\n",
    "done = wait([X_train, Y_train])\n",
    "\n",
    "\n",
    "dtrain = xgb.dask.DaskDMatrix(client, X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `dask_cudf.DataFrame.persist()` to ensure each GPU worker has ownership of data before training for optimal load-balance. Please note: this is optional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Parameters and Train with XGBoost\n",
    "\n",
    "The wall time output below indicates how long it took your GPU cluster to train an XGBoost model over the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 323 ms, sys: 80.8 ms, total: 403 ms\n",
      "Wall time: 2.47 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "params = {\n",
    "  'num_rounds':   100,\n",
    "  'max_depth':    8,\n",
    "  'max_leaves':   2**8,\n",
    "  'tree_method':  'gpu_hist',\n",
    "  'objective':    'reg:squarederror',\n",
    "  'grow_policy':  'lossguide'\n",
    "}\n",
    "\n",
    "## Optional: persist training data into memory\n",
    "# X_train = X_train.persist()\n",
    "# Y_train = Y_train.persist()\n",
    "\n",
    "trained_model = xgb.dask.train(client,params, dtrain, num_boost_round=params['num_rounds'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inputs for `xgb.dask.train`\n",
    "\n",
    "1. `client`: the `dask.distributed.Client`\n",
    "2. `params`: the training parameters for XGBoost. \n",
    "3. `dtrain`: an instance of `xgboost.dask.DaskDMatrix` containing our training input and target data.\n",
    "4. `num_boost_round=params['num_rounds']`: a specification on the number of boosting rounds for the training session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Predictions and the RMSE Metric for the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This distributed `xgb.dask.train` method returns a dictionary containing the resulting booster and evaluation history obtained from evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "booster = trained_model[\"booster\"] # \"Booster\" is the trained model\n",
    "history = trained_model['history'] # \"History\" is a dictionary containing evaluation results \n",
    "\n",
    "booster.set_param({'predictor': 'gpu_predictor'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nfs/rabhojwani/anaconda3/envs/rapids-env2/lib/python3.7/site-packages/distributed/worker.py:3378: UserWarning: Large object of size 16.24 MB detected in task graph: \n",
      "  [<function predict.<locals>.mapped_predict at 0x7f ... titions>, True]\n",
      "Consider scattering large objects ahead of time\n",
      "with client.scatter to reduce scheduler burden and \n",
      "keep data on workers\n",
      "\n",
      "    future = client.submit(func, big_data)    # bad\n",
      "\n",
      "    big_future = client.scatter(big_data)     # good\n",
      "    future = client.submit(func, big_future)  # good\n",
      "  % (format_bytes(len(b)), s)\n"
     ]
    }
   ],
   "source": [
    "pred = xgb.dask.predict(client,booster, X_test)\n",
    "true = Y_test['x']\n",
    "\n",
    "pred = pred.map_partitions(lambda part: cudf.Series(part))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: We mapped each partition of the result from `xgb.dask.predict` into `cudf.Series` to be able to substract it from `true` data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to run prediction via `xgb.dask.predict`\n",
    "\n",
    "1. `client`: the `dask.distributed.Client`\n",
    "2. `booster`: the Booster produced by the XGBoost training session\n",
    "3. `X_test`: an instance of `dask_cudf.core.DataFrame` containing the data to be inferenced (acquire predictions)\n",
    "\n",
    "`pred` and `true` are instances of `dask_cudf.core.Series` object. We align the index of both the Series by resetting it through `reset_index`. To compute the root mean squared error(RMSE) we first compute difference squared between the `pred` and `true` series through the operation `squared_error = ((pred - true)**2)`\n",
    "\n",
    "Finally, the mean is computed by using an aggregator from the `dask_cudf` API. The entire computation is initiated via `.compute()`. We take the square-root of the result, leaving us with `rmse = np.sqrt(squared_error.mean().compute())`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pred.reset_index(drop=True)\n",
    "true = true.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "squared_error = ((pred - true)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse value: 2.291584656127447\n"
     ]
    }
   ],
   "source": [
    "rmse = np.sqrt(squared_error.mean().compute())\n",
    "print('rmse value:', rmse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
